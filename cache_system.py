"""
SystÃ¨me de cache intelligent pour les prompts et rÃ©ponses mÃ©tÃ©orologiques.
Utilise la similaritÃ© sÃ©mantique pour Ã©viter de rÃ©gÃ©nÃ©rer des rÃ©ponses similaires.
AmÃ©liore les performances et rÃ©duit les coÃ»ts d'API.
"""

import json
import hashlib
import os
from typing import Dict, Optional, List, Tuple
from datetime import datetime, timedelta
import logging
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from sentence_transformers import SentenceTransformer

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WeatherCache:
    """
    Classe de cache intelligent pour les donnÃ©es mÃ©tÃ©orologiques.
    Utilise des embeddings pour dÃ©tecter les prompts similaires.
    """
    
    def __init__(self, cache_file: str = "weather_cache.json", similarity_threshold: float = 0.85):
        """
        Initialise le systÃ¨me de cache.
        
        Args:
            cache_file: Fichier de cache JSON
            similarity_threshold: Seuil de similaritÃ© pour considÃ©rer un prompt comme similaire
        """
        
        """
            cache_file: Fichier de cache JSON
            similarity_threshold: Seuil de similaritÃ© pour considÃ©rer un prompt comme similaire
        """

        self.cache_file = cache_file
        self.similarity_threshold = similarity_threshold
        self.cache_data = self._load_cache()
        self.embedding_model = None
        self._initialize_embedding_model()
        
    def _initialize_embedding_model(self):
        """Initialise le modÃ¨le d'embedding pour la similaritÃ© sÃ©mantique."""
        try:
            self.embedding_model = SentenceTransformer("Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2")
            logger.info("ModÃ¨le d'embedding initialisÃ© pour le cache")
        except Exception as e:
            logger.warning(f"Impossible d'initialiser le modÃ¨le d'embedding: {e}")
            self.embedding_model = None
    
    def _load_cache(self) -> Dict:
        """Charge le cache depuis le fichier JSON."""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                logger.info(f"Cache chargÃ©: {len(cache)} entrÃ©es")
                return cache
            else:
                logger.info("CrÃ©ation d'un nouveau cache")
                return {}
        except Exception as e:
            logger.error(f"Erreur lors du chargement du cache: {e}")
            return {}
    
    def _save_cache(self):
        """Sauvegarde le cache dans le fichier JSON."""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Cache sauvegardÃ©: {len(self.cache_data)} entrÃ©es")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du cache: {e}")
    
    def _generate_hash(self, prompt: str) -> str:
        """GÃ©nÃ¨re un hash pour le prompt."""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()
    
    def _embed_text(self, text: str) -> np.ndarray:
        """GÃ©nÃ¨re l'embedding d'un texte."""
        if self.embedding_model is None:
            return None
        try:
            embedding = self.embedding_model.encode([text])
            return embedding[0]
        except Exception as e:
            logger.error(f"Erreur lors de l'embedding: {e}")
            return None
    
    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calcule la similaritÃ© cosinus entre deux embeddings."""
        if embedding1 is None or embedding2 is None:
            return 0.0
        try:
            similarity = cosine_similarity([embedding1], [embedding2])[0][0]
            return float(similarity)
        except Exception as e:
            logger.error(f"Erreur lors du calcul de similaritÃ©: {e}")
            return 0.0
    
    def add_to_cache(self, prompt: str, response: str, metadata: Dict = None) -> None:
        """
        Ajoute une entrÃ©e au cache.
        
        Args:
            prompt: Le prompt original
            response: La rÃ©ponse gÃ©nÃ©rÃ©e
            metadata: MÃ©tadonnÃ©es supplÃ©mentaires (optionnel)
        """
        try:
            # GÃ©nÃ©ration du hash et de l'embedding
            prompt_hash = self._generate_hash(prompt)
            prompt_embedding = self._embed_text(prompt)
            
            # CrÃ©ation de l'entrÃ©e de cache
            cache_entry = {
                "prompt": prompt,
                "response": response,
                "prompt_hash": prompt_hash,
                "prompt_embedding": prompt_embedding.tolist() if prompt_embedding is not None else None,
                "timestamp": datetime.now().isoformat(),
                "metadata": metadata or {}
            }
            
            # Ajout au cache
            self.cache_data[prompt_hash] = cache_entry
            
            # Sauvegarde
            self._save_cache()
            
            logger.info(f"EntrÃ©e ajoutÃ©e au cache: {prompt_hash}")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'ajout au cache: {e}")
    
    def get_from_cache(self, prompt: str) -> Optional[str]:
        """
        RÃ©cupÃ¨re une rÃ©ponse du cache si elle existe.
        
        Args:
            prompt: Le prompt Ã  rechercher
            
        Returns:
            La rÃ©ponse du cache si trouvÃ©e, sinon None
        """
        try:
            # Recherche exacte par hash
            prompt_hash = self._generate_hash(prompt)
            if prompt_hash in self.cache_data:
                logger.info(f"Cache hit exact: {prompt_hash}")
                return self.cache_data[prompt_hash]["response"]
            
            # Recherche par similaritÃ© sÃ©mantique
            if self.embedding_model is not None:
                query_embedding = self._embed_text(prompt)
                if query_embedding is not None:
                    best_match = self._find_similar_prompt(query_embedding)
                    if best_match:
                        logger.info(f"Cache hit par similaritÃ©: {best_match['similarity']:.3f}")
                        return best_match["response"]
            
            logger.info("Cache miss")
            return None
            
        except Exception as e:
            logger.error(f"Erreur lors de la recherche dans le cache: {e}")
            return None
    
    def _find_similar_prompt(self, query_embedding: np.ndarray) -> Optional[Dict]:
        """
        Trouve le prompt le plus similaire dans le cache.
        
        Args:
            query_embedding: L'embedding du prompt de requÃªte
            
        Returns:
            L'entrÃ©e de cache la plus similaire si trouvÃ©e
        """
        best_match = None
        best_similarity = 0.0
        
        for cache_entry in self.cache_data.values():
            if cache_entry.get("prompt_embedding") is not None:
                cached_embedding = np.array(cache_entry["prompt_embedding"])
                similarity = self._calculate_similarity(query_embedding, cached_embedding)
                
                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_match = {
                        "response": cache_entry["response"],
                        "similarity": similarity,
                        "prompt": cache_entry["prompt"]
                    }
        
        return best_match
    
    def get_cache_stats(self) -> Dict:
        """Retourne les statistiques du cache."""
        total_entries = len(self.cache_data)
        total_size = os.path.getsize(self.cache_file) if os.path.exists(self.cache_file) else 0
        
        # Calcul de l'Ã¢ge moyen des entrÃ©es
        ages = []
        for entry in self.cache_data.values():
            if "timestamp" in entry:
                try:
                    timestamp = datetime.fromisoformat(entry["timestamp"])
                    age = datetime.now() - timestamp
                    ages.append(age.total_seconds() / 3600)  # En heures
                except Exception:
                    continue  # Ignore les entrÃ©es avec timestamp invalide
        avg_age = sum(ages) / len(ages) if ages else 0
        
        return {
            "total_entries": total_entries,
            "total_size_bytes": total_size,
            "average_age_hours": avg_age,
            "similarity_threshold": self.similarity_threshold
        }
    
    def clear_cache(self, older_than_days: int = None) -> int:
        """
        Nettoie le cache en supprimant les anciennes entrÃ©es.
        
        Args:
            older_than_days: Supprimer les entrÃ©es plus anciennes que X jours
            
        Returns:
            Nombre d'entrÃ©es supprimÃ©es
        """
        if older_than_days is None:
            # Suppression complÃ¨te
            removed_count = len(self.cache_data)
            self.cache_data = {}
        else:
            # Suppression des anciennes entrÃ©es
            cutoff_date = datetime.now() - timedelta(days=older_than_days)
            removed_count = 0
            
            keys_to_remove = []
            for key, entry in self.cache_data.items():
                timestamp = datetime.fromisoformat(entry["timestamp"])
                if timestamp < cutoff_date:
                    keys_to_remove.append(key)
                    removed_count += 1
            
            for key in keys_to_remove:
                del self.cache_data[key]
        
        self._save_cache()
        logger.info(f"Cache nettoyÃ©: {removed_count} entrÃ©es supprimÃ©es")
        return removed_count
    
    def search_cache(self, query: str, limit: int = 5) -> List[Dict]:
        """
        Recherche dans le cache par similaritÃ© sÃ©mantique.
        
        Args:
            query: RequÃªte de recherche
            limit: Nombre maximum de rÃ©sultats
            
        Returns:
            Liste des entrÃ©es les plus similaires
        """
        if self.embedding_model is None:
            return []
        
        query_embedding = self._embed_text(query)
        if query_embedding is None:
            return []
        
        results = []
        for entry in self.cache_data.values():
            if entry.get("prompt_embedding") is not None:
                cached_embedding = np.array(entry["prompt_embedding"])
                similarity = self._calculate_similarity(query_embedding, cached_embedding)
                
                results.append({
                    "prompt": entry["prompt"],
                    "response": entry["response"],
                    "similarity": similarity,
                    "timestamp": entry["timestamp"]
                })
        
        # Tri par similaritÃ© dÃ©croissante
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:limit]


# Fonction utilitaire pour intÃ©gration facile
def get_cached_response(prompt: str, cache_file: str = "weather_cache.json") -> Optional[str]:
    """
    Fonction utilitaire pour rÃ©cupÃ©rer une rÃ©ponse du cache.
    
    Args:
        prompt: Le prompt Ã  rechercher
        cache_file: Fichier de cache Ã  utiliser
        
    Returns:
        La rÃ©ponse du cache si trouvÃ©e, sinon None
    """
    cache = WeatherCache(cache_file)
    return cache.get_from_cache(prompt)


def add_to_cache(prompt: str, response: str, cache_file: str = "weather_cache.json", metadata: Dict = None) -> None:
    """
    Fonction utilitaire pour ajouter une entrÃ©e au cache.
    
    Args:
        prompt: Le prompt original
        response: La rÃ©ponse gÃ©nÃ©rÃ©e
        cache_file: Fichier de cache Ã  utiliser
        metadata: MÃ©tadonnÃ©es supplÃ©mentaires
    """
    cache = WeatherCache(cache_file)
    cache.add_to_cache(prompt, response, metadata)


# Test du systÃ¨me de cache
if __name__ == "__main__":
    # Test du systÃ¨me de cache
    cache = WeatherCache()
    
    # Ajout d'entrÃ©es de test
    test_prompts = [
        "Ø£Ù†Øª Ù…Ø°ÙŠØ¹ Ù†Ø´Ø±Ø© Ø¬ÙˆÙŠØ© Ù…Ø­ØªØ±Ù. Casablanca 22 Ciel_Clair",
        "Ø£Ù†Øª Ù…Ø°ÙŠØ¹ Ù†Ø´Ø±Ø© Ø¬ÙˆÙŠØ© Ù…Ø­ØªØ±Ù. Rabat 21 Peu_Nuageux",
        "Ø£Ù†Øª Ù…Ø°ÙŠØ¹ Ù†Ø´Ø±Ø© Ø¬ÙˆÙŠØ© Ù…Ø­ØªØ±Ù. Marrakech 28 Ciel_Clair"
    ]
    
    test_responses = [
        "Ø£Ø³Ø¹Ø¯ Ø§Ù„Ù„Ù‡ Ø£ÙˆÙ‚Ø§ØªÙƒÙ… Ø¨ÙƒÙ„ Ø®ÙŠØ±ØŒ Ø¥Ù„ÙŠÙƒÙ… Ø§Ù„Ù†Ø´Ø±Ø© Ø§Ù„Ø¬ÙˆÙŠØ©...",
        "Ø£Ù‡Ù„Ø§Ù‹ ÙˆÙ…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ Ù†Ø´Ø±ØªÙ†Ø§ Ø§Ù„Ø¬ÙˆÙŠØ©...",
        "Ù†Ø±Ø­Ø¨ Ø¨ÙƒÙ… ÙÙŠ Ù†Ø´Ø±ØªÙ†Ø§ Ø§Ù„Ø¬ÙˆÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…..."
    ]
    
    for prompt, response in zip(test_prompts, test_responses):
        cache.add_to_cache(prompt, response)
    
    # Test de rÃ©cupÃ©ration
    test_query = "Ø£Ù†Øª Ù…Ø°ÙŠØ¹ Ù†Ø´Ø±Ø© Ø¬ÙˆÙŠØ© Ù…Ø­ØªØ±Ù. Casablanca 22 Ciel_Clair"
    cached_response = cache.get_from_cache(test_query)
    
    if cached_response:
        print(f"âœ… RÃ©ponse trouvÃ©e dans le cache: {cached_response[:50]}...")
    else:
        print("âŒ Aucune rÃ©ponse trouvÃ©e dans le cache")
    
    # Statistiques du cache
    stats = cache.get_cache_stats()
    print(f"ğŸ“Š Statistiques du cache: {stats}") 