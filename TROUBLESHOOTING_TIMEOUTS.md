# Guide de D√©pannage - Timeouts Ollama

## üö® Probl√®me : Timeout avec Ollama

### Sympt√¥mes
```
Error generating script with Ollama: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=180)
```

### ‚úÖ Solutions Impl√©ment√©es

#### 1. Prompt Raccourci
Le fichier `prompt_weather.txt` a √©t√© optimis√© :
- **Avant** : ~2000 caract√®res
- **Apr√®s** : ~500 caract√®res
- **R√©duction** : 75% de la taille

#### 2. Contexte RAG Optimis√©
- **Nombre d'exemples** : R√©duit de 3 √† 2
- **Longueur des exemples** : Limit√© √† 100 caract√®res
- **Format** : Simplifi√© et concis

#### 3. Param√®tres de Recherche
```python
# Dans app.py
top_k=2  # Au lieu de 3

# Dans rag_integration.py
context_summary = "\n".join([f"- {context[:100]}..." for context in rag_context[:2]])
```

### üîß Optimisations Suppl√©mentaires

#### 1. V√©rifier la Configuration Ollama
```bash
# V√©rifier que Ollama fonctionne
curl http://localhost:11434/api/tags

# Tester avec un prompt simple
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "prompt": "Bonjour, comment allez-vous?",
    "stream": false
  }'
```

#### 2. Augmenter le Timeout (si n√©cessaire)
Dans `script.py`, vous pouvez augmenter le timeout :
```python
# Augmenter le timeout si n√©cessaire
response = requests.post(url, json=data, timeout=300)  # 5 minutes
```

#### 3. R√©duire la Complexit√© du Mod√®le
Si les timeouts persistent, utilisez un mod√®le plus l√©ger :
```python
# Dans script.py, changer le mod√®le
data = {
    "model": "llama2:7b",  # Mod√®le plus l√©ger
    "prompt": prompt,
    "stream": False
}
```

### üìä Monitoring des Performances

#### Test de Performance
```bash
python test_optimized_rag.py
```

#### M√©triques √† Surveiller
- **Temps de g√©n√©ration** : < 60 secondes
- **Longueur du prompt** : < 1500 caract√®res
- **Nombre d'exemples RAG** : 2 maximum

### üö® D√©pannage Avanc√©

#### 1. Probl√®me de M√©moire
```bash
# V√©rifier l'utilisation m√©moire
htop
# ou
free -h

# Red√©marrer Ollama si n√©cessaire
sudo systemctl restart ollama
```

#### 2. Probl√®me de R√©seau
```bash
# Tester la connectivit√©
ping localhost
telnet localhost 11434
```

#### 3. Probl√®me de Mod√®le
```bash
# Lister les mod√®les disponibles
ollama list

# T√©l√©charger un mod√®le plus l√©ger
ollama pull llama2:7b
```

### üîÑ Fallback Automatique

Le syst√®me inclut un fallback automatique :
```python
try:
    # Tentative avec RAG
    enhanced_prompt = rag_generator.enhance_prompt_with_rag(...)
except Exception as e:
    # Fallback vers prompt simple
    enhanced_prompt = prompt.replace("{data}", weather_data)
```

### üìà Optimisations Futures

#### 1. Cache des Embeddings
```python
# Sauvegarder les embeddings
rag_system.save_embeddings("embeddings_cache.npy")

# Charger les embeddings
rag_system.load_embeddings("embeddings_cache.npy")
```

#### 2. Quantification du Mod√®le
```bash
# Utiliser un mod√®le quantifi√©
ollama pull llama2:7b-q4_0
```

#### 3. Parall√©lisation
```python
# Traitement parall√®le des embeddings
from concurrent.futures import ThreadPoolExecutor
```

### ‚úÖ Checklist de V√©rification

- [ ] Ollama fonctionne sur le port 11434
- [ ] Le mod√®le est t√©l√©charg√© et accessible
- [ ] La m√©moire syst√®me est suffisante
- [ ] Le prompt est < 1500 caract√®res
- [ ] Le nombre d'exemples RAG est ‚â§ 2
- [ ] Le timeout est configur√© √† 180s minimum

### üÜò En Cas d'Urgence

Si les timeouts persistent :

1. **D√©sactiver temporairement le RAG** :
   ```python
   # Dans app.py, commenter la section RAG
   # enhanced_prompt = st.session_state.rag_generator.enhance_prompt_with_rag(...)
   enhanced_prompt = prompt.replace("{data}", st.session_state.extracted_text)
   ```

2. **Utiliser un prompt minimal** :
   ```python
   minimal_prompt = f"ÿ£ŸÜÿ™ ŸÖÿ∞Ÿäÿπ ŸÜÿ¥ÿ±ÿ© ÿ¨ŸàŸäÿ©. {weather_data}"
   ```

3. **Augmenter les ressources** :
   - Plus de RAM
   - CPU plus puissant
   - Mod√®le plus l√©ger

### üìû Support

En cas de probl√®me persistant :
1. V√©rifiez les logs Ollama : `ollama logs`
2. Testez avec un prompt simple
3. V√©rifiez la configuration syst√®me
4. Consultez la documentation Ollama

---

**Note** : Ces optimisations maintiennent la qualit√© tout en r√©duisant significativement les risques de timeout. 